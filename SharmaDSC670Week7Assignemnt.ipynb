{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7b95164-6963-4658-ba23-867bfd9bf8bd",
   "metadata": {},
   "source": [
    "## Assignment Week 7\n",
    "\n",
    "### Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5cee83-c623-463b-8456-3e7b9fb39d48",
   "metadata": {},
   "source": [
    "LLM fine-tuning is a technique to adjust the wights of a LLM to make it generate tailored response for a specific task without the hassle of training and building a LLM from scratch. Fine tuning requires preparation of the task specific dataset, fine tune the model and then testing the performance of the fine-tuned model.\n",
    "\n",
    "``` Prepare dataset >> Load dataset to OpenAI >> Start Fine Tuning Job >> Run test```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5484fa56-3308-49d0-9683-432e08128888",
   "metadata": {},
   "source": [
    "As part of the assignment, Open AI `gpt-4o-mini` will be fine-tuned model so that it can act as an assistant which replies answers with emojis but instead of emoji it should reply **with words**.\n",
    "\n",
    "For example, if we ask about about a smiling face instead of üòÄ, it must respond with `(grinningface)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9023fb-878f-4256-a061-06da4de87580",
   "metadata": {},
   "source": [
    "### 1. Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374f6fa6-d9d7-47b4-b236-3ca4e371c913",
   "metadata": {},
   "source": [
    "Open AI fine tuning service expects `jsonl` format with each line containing example with messages similar to [chat completion api format][1] (example below). It is a list of messages where each message has a role, content, and optional name. \n",
    "\n",
    "```\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"}, {\"role\": \"assistant\", \"content\": \"Oh, just some guy named William Shakespeare. Ever heard of him?\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, {\"role\": \"assistant\", \"content\": \"Around 384,400 kilometers. Give or take a few, like that really matters.\"}]}\n",
    "```\n",
    "Data prepration is not required for this assignment because we are using already curated dataset from the author ([Bahree, 2024][2]) that follows the above jsonl format.\n",
    "\n",
    "[1]:https://platform.openai.com/docs/api-reference/chat/createm\n",
    "[2]:https://github.com/bahree/GenAIBook/tree/main/chapters/ch09/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed9b6b32-abd9-4f9c-b875-98ba1469b671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import tiktoken  \n",
    "import os\n",
    "import pprint\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddd5dee0-7959-4fd9-a85b-f768ab69150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 65536\n",
    "\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 10000\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1ecbbe1-bf5d-4ad8-8631-72a397bb99df",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_file = \"./datasets/emoji_ft_train.jsonl\"\n",
    "validation_data_file = \"./datasets/emoji_ft_validation.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b0b5e1-fdb1-41d0-a901-704fa65dd768",
   "metadata": {},
   "source": [
    "Fine tuning is billed by the `number of tokens` used in the training examples. Hence, it is always advisable to estimate the numbers tokens in the training examples and how many total tokens will be used in the fine-tuning. This will provide a rough estimate about the cost of the fine-tuning to avoid any surprises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61401c1c-4e99-4bd8-a589-53c182332746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token encoding for gpt-4o-mini is : o200k_base\n"
     ]
    }
   ],
   "source": [
    "# get the encoding for gpt-4o-mini-2024-07-18\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini-2024-07-18\")\n",
    "print(f\"Token encoding for gpt-4o-mini is : {encoding.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d26561fb-e084-43c4-9caf-02b28640257a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of tokens in the messages\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "# print the number of tokens in the assistant messages\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c6d1b24-0a30-4ac2-94a2-63d4489fe0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the number of tokens that will be charged for during training\n",
    "def estimate_tokens(dataset, assistant_tokens):\n",
    "    # Set the initial number of epochs to the target epochs\n",
    "    n_epochs = TARGET_EPOCHS\n",
    "    \n",
    "    # Get the number of examples in the dataset\n",
    "    n_train_examples = len(dataset)\n",
    "\n",
    "    # If the examples total is less than the minimum target\n",
    "    # adjust the epochs to ensure we have enough examples for\n",
    "    # training\n",
    "    if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "        n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "    \n",
    "    # If the  number of examples is more than the maximum target\n",
    "    # adjust the  epochs to ensure we don't exceed the maximum \n",
    "    # for training\n",
    "    elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "        n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "    # Calculate the total number of tokens in the dataset\n",
    "    # n_billing_tokens_in_dataset = sum(min(MAX_TOKENS, length) for length in assistant_tokens)\n",
    "    n_billing_tokens_in_dataset = sum(min(MAX_TOKENS, length) for length in assistant_tokens)\n",
    "\n",
    "    # Print the total token count that will be charged during training\n",
    "    print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "\n",
    "    # Print the default number of epochs for training\n",
    "    print(f\"You will train for {n_epochs} epochs on this dataset\")\n",
    "\n",
    "    # Print the total number of tokens that will be charged during training\n",
    "    print(f\"You will be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
    "\n",
    "    # If the total token count exceeds the maximum tokens, print a warning \n",
    "    if n_billing_tokens_in_dataset > MAX_TOKENS:\n",
    "        print(f\"WARNING: Your dataset contains examples longer than 4K tokens by {n_billing_tokens_in_dataset - MAX_TOKENS} tokens.\")\n",
    "        print(\"You will be charged for the full length of these examples during training, but only the first 4K tokens will be used for training.\")\n",
    "    return n_billing_tokens_in_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87a620c3-37f0-4626-aaf4-66e0a9b19e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(training_data_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    train_dataset = [json.loads(line) for line in f]\n",
    "with open(validation_data_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    valid_dataset = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e767e92-e90d-49a8-9aa2-c1bca7f90adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tokens = []\n",
    "assistant_tokens = []\n",
    "for example in train_dataset:\n",
    "    messages = example.get(\"messages\", {})\n",
    "    total_tokens.append(num_tokens_from_messages(messages))\n",
    "    assistant_tokens.append(num_assistant_tokens_from_messages(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40bfe173-93ee-49df-9529-5a6740c245fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has ~19522 tokens that will be charged for during training\n",
      "You will train for 3 epochs on this dataset\n",
      "You will be charged for ~58566 tokens\n"
     ]
    }
   ],
   "source": [
    "tokens_used_in_traiing = estimate_tokens(train_dataset, total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e957153-7710-4103-aa39-7f9a189c067d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Cost: $ 0.175698\n"
     ]
    }
   ],
   "source": [
    "print(f\"Estimated Cost: $ {(3/1000000)*tokens_used_in_traiing*TARGET_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329030b1-d793-4a3c-92de-c711324ce75a",
   "metadata": {},
   "source": [
    "OpenAI doesnot charge for validation tokens, so only training tokens are charged. Current pricing is \\$3 per 1M tokens. For the above training dataset the estimated cost is `$0.18`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bee75c-c35a-48f6-aeff-57a033446aee",
   "metadata": {},
   "source": [
    "### 2. Upload training file to Open AI\n",
    "\n",
    "Open AI requires the training and validation file to be uploaded in OpenAI file storage for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4e765170-5f9c-4ea6-9765-49f9a07f35c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get api key from file\n",
    "with open(\"../../apikeys/openai-keys.json\", \"r\") as key_file:\n",
    "    api_key = json.load(key_file)[\"default_api_key\"]\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1d6f9896-9c71-42a2-9be8-db9c915c5dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload training file\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.files.create(\n",
    "  file=open(training_data_file, \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "17f71997-f71d-4a4c-9439-39a45e1670d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file details\n",
      " FileObject(id='file-LXx1cigU9wSjnU7urUBiUb', bytes=119751, created_at=1737307365, filename='emoji_ft_train.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)\n"
     ]
    }
   ],
   "source": [
    "print(\"Uploaded file details\\n\",response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "aa0e8769-dc3f-4ee6-a26f-0c3439977bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file details\n",
      " FileObject(id='file-KE6JVSUfZSWx8ShsRu1sca', bytes=2053, created_at=1737308416, filename='emoji_ft_validation.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)\n"
     ]
    }
   ],
   "source": [
    "# upload validation file\n",
    "validation_file_response = client.files.create(\n",
    "  file=open(validation_data_file, \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "print(\"Uploaded file details\\n\",validation_file_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df56717e-f99d-4958-800b-edbef48e08cd",
   "metadata": {},
   "source": [
    "### 3. Create and start Fine tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ac5c9e12-e219-4eef-a346-ea4134d551c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuning job details:\n",
      " FineTuningJob(id='ftjob-c2mZjQSpX7Kd2rrhwnxNOKw2', created_at=1737308522, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size='auto', learning_rate_multiplier='auto', n_epochs=3), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-i0HGnyzXyYOGEdOd7p2dDDri', result_files=[], seed=918569459, status='validating_files', trained_tokens=None, training_file='file-LXx1cigU9wSjnU7urUBiUb', validation_file='file-KE6JVSUfZSWx8ShsRu1sca', estimated_finish=None, integrations=[], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size='auto', learning_rate_multiplier='auto', n_epochs=3)), type='supervised'), user_provided_suffix='emoji-20240122')\n"
     ]
    }
   ],
   "source": [
    "# startting fine tuning job with 3 epochs\n",
    "ft_job = client.fine_tuning.jobs.create(\n",
    "    training_file=\"file-LXx1cigU9wSjnU7urUBiUb\",\n",
    "    validation_file=\"file-KE6JVSUfZSWx8ShsRu1sca\",\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    suffix=\"emoji-20240122\",\n",
    "    hyperparameters={\n",
    "        \"n_epochs\":3\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"Finetuning job details:\\n\", ft_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "94ba005a-227d-425d-b497-a0283e04b7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ftjob-c2mZjQSpX7Kd2rrhwnxNOKw2 running\n",
      "ftjob-c2mZjQSpX7Kd2rrhwnxNOKw2 running\n",
      "ftjob-c2mZjQSpX7Kd2rrhwnxNOKw2 running\n",
      "ftjob-c2mZjQSpX7Kd2rrhwnxNOKw2 running\n",
      "ftjob-c2mZjQSpX7Kd2rrhwnxNOKw2 succeeded\n",
      "Job Status: succeeded\n",
      "Elapsed time: 97 minutes 1 seconds\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the state of a fine-tune\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    job = client.fine_tuning.jobs.retrieve(\"ftjob-c2mZjQSpX7Kd2rrhwnxNOKw2\")\n",
    "    print(job.id, job.status)\n",
    "    if job.status in [\"succeeded\",\"failed\",\"cancelled\"]:\n",
    "        print(f\"Job Status: {job.status}\")\n",
    "        print(\"Elapsed time: {} minutes {} seconds\".format(int((time.time() - start_time) // 60), int((time.time() - start_time) % 60)))\n",
    "        break\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1020fa7-c90e-4327-9762-47973e02e696",
   "metadata": {},
   "source": [
    "### 3. Testing the fine tuned model\n",
    "\n",
    "Now, as the fine tuning is completed, the fine tuned model can be used for the task. For testing the fine-tuned model, same prompts are sent the foundational model and also the fine-tuned model, then the responses are checked below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835a00dd-971f-4f38-91ca-661c42ddcde2",
   "metadata": {},
   "source": [
    "Prompt 1: Enjoying snowfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8953a7c-38e4-4d8f-8f44-e5a17a9ed9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get api key from file\n",
    "with open(\"../../apikeys/openai-keys.json\", \"r\") as key_file:\n",
    "    api_key = json.load(key_file)[\"default_api_key\"]\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b608d71-520b-4de3-b4a2-01abe1064d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f7c71b-8206-4ce8-88dc-7151af520c45",
   "metadata": {},
   "source": [
    "#### GPT 4o mini without fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebfc61a1-c8a6-4053-bade-1bec22a37453",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": f\"You are a assistant that generates emojis\\n\\\n",
    "            If you donot know the answer, simply state that you donot know.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": f\"Q: Enjoying snowfall\\nA:\"\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60e40a71-159d-4a0c-bdc5-fefd32fc10d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from GPT 4o mini without fine-tuning:\n",
      " ‚ùÑÔ∏è‚òÉÔ∏èüå®Ô∏è‚ú®\n"
     ]
    }
   ],
   "source": [
    "chat_response_1_std = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages\n",
    ")\n",
    "print(f\"Response from GPT 4o mini without fine-tuning:\\n {chat_response_1_std.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1385aba7-f786-4659-a94c-353736ad54ce",
   "metadata": {},
   "source": [
    "#### Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87770a71-d01f-40a1-8ab3-be64c0902e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from GPT 4o mini without fine-tuning:\n",
      " (cloudwithsnow)\n"
     ]
    }
   ],
   "source": [
    "chat_response_1_fine_tuned = client.chat.completions.create(\n",
    "    model=\"ft:gpt-4o-mini-2024-07-18:personal:emoji-20240122:ArUCgSFs\",\n",
    "    messages=messages\n",
    ")\n",
    "print(f\"Response from GPT 4o mini without fine-tuning:\\n {chat_response_1_fine_tuned.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f28bbeb-9b3e-4c3e-9c21-1510933e6391",
   "metadata": {},
   "source": [
    "The fined-tuned model responded with text `(cloudwithsnow)` instead of an actual emoji. In contrast the foundational model responded to the same prompt with moji ‚ùÑÔ∏è‚òÉÔ∏èüå®Ô∏è‚ú®."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52d60040-3360-4580-a11b-dc03e953ec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get api key from file\n",
    "with open(\"../../apikeys/openai-keys.json\", \"r\") as key_file:\n",
    "    api_key = json.load(key_file)[\"default_api_key\"]\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e47d9e59-ac7c-4604-bb4e-41143992864a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e6cda6-5143-4e5b-93e0-84434137370e",
   "metadata": {},
   "source": [
    "Prompt 2: Celebrating graduation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df26105b-2d47-45ea-9af7-37bf07a47df8",
   "metadata": {},
   "source": [
    "#### GPT 4o mini without fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b77f409d-db14-49b8-8f76-ceb696f707d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": f\"You are a assistant that generates emojis\\n\\\n",
    "            If you donot know the answer, simply state that you donot know.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": f\"Q: Celebrating graduation\\nA:\"\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe0411e5-6f66-4f89-aaba-3ebf1b50dd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from GPT 4o mini without fine-tuning:\n",
      " üéìüéâüéäü•≥üìö‚ú®\n"
     ]
    }
   ],
   "source": [
    "chat_response_2_std = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages\n",
    ")\n",
    "print(f\"Response from GPT 4o mini without fine-tuning:\\n {chat_response_2_std.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1028f6-eb00-46dc-aaeb-dd62acd65595",
   "metadata": {},
   "source": [
    "#### Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffea543b-536f-401a-8977-a0260a17abc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from GPT 4o mini without fine-tuning:\n",
      " (graduationcap)\n"
     ]
    }
   ],
   "source": [
    "chat_response_2_fine_tuned = client.chat.completions.create(\n",
    "    model=\"ft:gpt-4o-mini-2024-07-18:personal:emoji-20240122:ArUCgSFs\",\n",
    "    messages=messages\n",
    ")\n",
    "print(f\"Response from the fine-tuning model:\\n {chat_response_2_fine_tuned.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa299e6-5c2b-42db-a144-ec2bb19253f3",
   "metadata": {},
   "source": [
    "The fined-tuned model responded with text `(graduationcap)` instead of an actual emoji. In contrast the foundational model responded to the same prompt with moji üéìüéâüéäü•≥üìö‚ú®.  \n",
    "\n",
    "Results from above testing with two prompts demonstrates that the fine-tuning is working as expected and responding with text description of emojis. However, it is interesting to notice that the _foundation model response has **multiple emojis**_ whereas the _fine-tuned model response has text descrption of **only** one emoji_, which is mostly be due to the fact that the training data has examples with text dscription of only one emoji. Depending on the nature of the business problem, multiple emojis may describe the emotion better. This highlights the fact that fine-tuning may also inadvarently change the response of the foundational model caused by effect as `catastrophic forgetting` which might have adverse effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed80f306-b2bc-41cd-a029-0427f2668f53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
